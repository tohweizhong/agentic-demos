{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to create a custom-trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then get a prediction from the deployed model by sending data.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- BigQuery\n",
    "- Cloud Storage\n",
    "- Vertex AI managed Datasets\n",
    "- Vertex AI Training\n",
    "- Vertex AI Endpoints\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex AI custom `TrainingPipeline` for training a model.\n",
    "- Train a TensorFlow model.\n",
    "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
    "- Make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). For this tutorial, you use only the fields `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, `body_mass_g` from the dataset to predict the penguins species (`species`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Cloud Storage, Bigquery and Vertex AI SDKs for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (1.60.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (2.19.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery[pandas] in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.29.0)\n",
      "Collecting google-cloud-bigquery[pandas]\n",
      "  Downloading google_cloud_bigquery-3.39.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.28.1)\n",
      "Collecting google-auth<3.0.0,>=2.45.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_auth-2.45.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.1.2)\n",
      "Collecting google-genai<2.0.0,>=1.37.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_genai-1.56.0-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.12.5)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: docstring_parser<1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.17.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.63.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (4.9.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.9.0.post0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.12.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery[pandas]) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (1.26.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.3.3)\n",
      "Collecting pandas-gbq>=0.26.1 (from google-cloud-bigquery[pandas])\n",
      "  Downloading pandas_gbq-0.32.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (9.0.0)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas>=1.3.0->google-cloud-bigquery[pandas]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas>=1.3.0->google-cloud-bigquery[pandas]) (2025.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (80.9.0)\n",
      "Collecting pydata-google-auth>=1.5.0 (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas])\n",
      "  Downloading pydata_google_auth-1.9.1-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting psutil>=5.9.8 (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas])\n",
      "  Downloading psutil-7.2.0-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
      "Collecting google-auth-oauthlib>=0.7.0 (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas])\n",
      "  Downloading google_auth_oauthlib-1.2.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "INFO: pip is looking at multiple versions of google-auth-oauthlib to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from google-auth-oauthlib>=0.7.0->pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (3.3.1)\n",
      "Downloading google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Downloading google_auth-2.45.0-py2.py3-none-any.whl (233 kB)\n",
      "Downloading google_genai-1.56.0-py3-none-any.whl (426 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading google_cloud_bigquery-3.39.0-py3-none-any.whl (259 kB)\n",
      "Downloading pandas_gbq-0.32.0-py3-none-any.whl (46 kB)\n",
      "Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading psutil-7.2.0-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (154 kB)\n",
      "Downloading pydata_google_auth-1.9.1-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: psutil, protobuf, distro, google-auth, google-auth-oauthlib, pydata-google-auth, google-genai, google-cloud-storage, google-cloud-bigquery, pandas-gbq, google-cloud-aiplatform\n",
      "\u001b[2K  Attempting uninstall: psutil\n",
      "\u001b[2K    Found existing installation: psutil 5.9.3\n",
      "\u001b[2K    Uninstalling psutil-5.9.3:\n",
      "\u001b[2K      Successfully uninstalled psutil-5.9.3\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 3.19.6\n",
      "\u001b[2K    Uninstalling protobuf-3.19.6:\n",
      "\u001b[2K      Successfully uninstalled protobuf-3.19.6━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: google-auth━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: google-auth 2.43.0━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling google-auth-2.43.0:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.43.0━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: google-auth-oauthlib━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/11\u001b[0m [google-auth]\n",
      "\u001b[2K    Found existing installation: google-auth-oauthlib 0.4.6━━━\u001b[0m \u001b[32m 3/11\u001b[0m [google-auth]\n",
      "\u001b[2K    Uninstalling google-auth-oauthlib-0.4.6:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/11\u001b[0m [google-auth]\n",
      "\u001b[2K      Successfully uninstalled google-auth-oauthlib-0.4.6━━━━━\u001b[0m \u001b[32m 3/11\u001b[0m [google-auth]\n",
      "\u001b[2K  Attempting uninstall: google-cloud-storage[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [google-genai]\n",
      "\u001b[2K    Found existing installation: google-cloud-storage 2.19.0━━\u001b[0m \u001b[32m 6/11\u001b[0m [google-genai]\n",
      "\u001b[2K    Uninstalling google-cloud-storage-2.19.0:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [google-genai]\n",
      "\u001b[2K      Successfully uninstalled google-cloud-storage-2.19.0━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [google-genai]\n",
      "\u001b[2K  Attempting uninstall: google-cloud-bigquery0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [google-cloud-storage]\n",
      "\u001b[2K    Found existing installation: google-cloud-bigquery 3.29.0━\u001b[0m \u001b[32m 7/11\u001b[0m [google-cloud-storage]\n",
      "\u001b[2K    Uninstalling google-cloud-bigquery-3.29.0:0m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [google-cloud-storage]\n",
      "\u001b[2K      Successfully uninstalled google-cloud-bigquery-3.29.0━━━\u001b[0m \u001b[32m 7/11\u001b[0m [google-cloud-storage]\n",
      "\u001b[2K  Attempting uninstall: google-cloud-aiplatform\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m 8/11\u001b[0m [google-cloud-bigquery]\n",
      "\u001b[2K    Found existing installation: google-cloud-aiplatform 1.60.0[0m \u001b[32m 8/11\u001b[0m [google-cloud-bigquery]\n",
      "\u001b[2K    Uninstalling google-cloud-aiplatform-1.60.0:[90m━━━━━━━━━━\u001b[0m \u001b[32m 8/11\u001b[0m [google-cloud-bigquery]\n",
      "\u001b[2K      Successfully uninstalled google-cloud-aiplatform-1.60.00m━━━\u001b[0m \u001b[32m10/11\u001b[0m [google-cloud-aiplatform]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [google-cloud-aiplatform]-cloud-aiplatform]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.8 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.8 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.25.8 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.8 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 3.7.0 which is incompatible.\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorboard 2.11.2 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.2.2 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed distro-1.9.0 google-auth-2.45.0 google-auth-oauthlib-1.2.2 google-cloud-aiplatform-1.132.0 google-cloud-bigquery-3.39.0 google-cloud-storage-3.7.0 google-genai-1.56.0 pandas-gbq-0.32.0 protobuf-4.25.8 psutil-7.2.0 pydata-google-auth-1.9.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'\n",
    "\n",
    "#automatically restarts kernel\n",
    "import IPython \n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a47846030fef"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3c8049930470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT=!(gcloud config get-value project)\n",
    "PROJECT_ID=PROJECT[0]\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54f9d7c1876",
    "tags": []
   },
   "source": [
    "#### Region\n",
    "\n",
    "Set the `REGION` variable as per the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3aaadaaf9b30"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"qwiklabs-gcp-02-4aecd326bf7c-cymbal\" # update it from the lab instructions\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "750d53e37094"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c9d3ac73dfbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.instance_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.instance_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.params_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.params_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.predict.prediction_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.predict.prediction_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1.schema.trainingjob.definition_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1.schema.trainingjob.definition_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.instance_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.instance_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.params_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.params_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.predict.prediction_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.predict.prediction_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.aiplatform.v1beta1.schema.trainingjob.definition_v1beta1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.aiplatform.v1beta1.schema.trainingjob.definition_v1beta1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c163842eabd"
   },
   "source": [
    "### Initialize BigQuery Client\n",
    "\n",
    "Initialize the BigQuery Python client for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fad2ba1ad7c3"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Set up BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a2c41bc91a6"
   },
   "source": [
    "## Task 4. Create a Vertex AI Tabular Dataset from the BigQuery dataset\n",
    "\n",
    "### Preprocess data and split data\n",
    "- Convert categorical features to numeric\n",
    "- Split train and test data in the fration 80-20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e3a2449cfcf1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2619: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2633: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2647: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_COLUMN = \"species\"\n",
    "\n",
    "# Define the BigQuery source dataset\n",
    "BQ_SOURCE = \"bigquery-public-data.ml_datasets.penguins\"\n",
    "\n",
    "# Define NA values\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Download a table\n",
    "table = bq_client.get_table(BQ_SOURCE)\n",
    "df = bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Drop unusable rows\n",
    "df = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df[\"island\"], _ = pd.factorize(df[\"island\"])\n",
    "df[\"species\"], _ = pd.factorize(df[\"species\"])\n",
    "df[\"sex\"], _ = pd.factorize(df[\"sex\"])\n",
    "\n",
    "# Split into a training and holdout dataset\n",
    "df_train = df.sample(frac=0.8, random_state=100)\n",
    "df_holdout = df[~df.index.isin(df_train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a39df4692a70"
   },
   "source": [
    "### Create a Vertex AI Tabular Dataset\n",
    "\n",
    "Create a Vertex AI tabular dataset resource from BigQuery training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7fa452ee5c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/251597415322/locations/us-central1/datasets/4887106534359695360/operations/7287609162488348672\n",
      "TabularDataset created. Resource name: projects/251597415322/locations/us-central1/datasets/4887106534359695360\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/251597415322/locations/us-central1/datasets/4887106534359695360')\n",
      "Created dataset: projects/251597415322/locations/us-central1/datasets/4887106534359695360\n"
     ]
    }
   ],
   "source": [
    "# Create BigQuery dataset\n",
    "BQ_DATASET=\"cymbal_penguins_dataset\"\n",
    "bq_dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
    "bq_client.create_dataset(bq_dataset, exists_ok=True)\n",
    "\n",
    "# Create a Vertex AI tabular dataset from BigQuery training data\n",
    "df_source=df_train\n",
    "staging_path=\"cymbal_penguins_table\"\n",
    "display_name=\"cymbal_penguins\"\n",
    "\n",
    "#[ TODO - Insert your code ]\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define the BigQuery table for your training data\n",
    "BQ_TABLE_TRAIN = \"cymbal_penguins_table\"\n",
    "table_id_train = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_TRAIN}\"\n",
    "\n",
    "# Load the training DataFrame to BigQuery\n",
    "job = bq_client.load_table_from_dataframe(df_train, table_id_train)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "# Create a Vertex AI tabular dataset from the BigQuery table\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=display_name,\n",
    "    bq_source=f\"bq://{table_id_train}\",\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "print(f\"Created dataset: {dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "## Task 5. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "# Define the command args for the training script\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--label_column=\" + LABEL_COLUMN,\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### Training script\n",
    "\n",
    "Complete the contents of the training script, `task.py`. You need to write code in the **[ TODO - Insert your code ]** section by training the model with epochs and batch size according and saves the trained model artifact to Cloud Storage directory `aiplatform-custom-training` in the created Cloud Storage Bucket location using `os.environ['AIP_MODEL_DIR']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "72rUqXNFlugx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label_column', required=True, type=str)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=10, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = args.label_column\n",
    "\n",
    "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
    "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "        \n",
    "    # Download the BigQuery table as a dataframe\n",
    "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
    "    table = bq_client.get_table(bq_table_uri)\n",
    "    return bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Download dataset splits\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "):\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\n",
    "    y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\n",
    "    x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    num_species = len(df_train_y.unique())\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),            \n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "dataset_validation = dataset_validation.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### Executes script in Cloud Vertex AI Training\n",
    "\n",
    "Define your custom `TrainingPipeline` on Vertex AI.\n",
    "\n",
    "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
    "\n",
    "- `display_name`: The user-defined name of this training pipeline.\n",
    "- `script_path`: The local path to the training script.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a pre-built container or a custom container.\n",
    "\n",
    "Use the `run` function to start training.\n",
    "\n",
    "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://qwiklabs-gcp-02-4aecd326bf7c-cymbal/aiplatform-2025-12-24-07:53:03.411-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://qwiklabs-gcp-02-4aecd326bf7c-cymbal/aiplatform-custom-training-2025-12-24-07:53:03.534 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3159232034308620288?project=251597415322\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7475985053701373952?project=251597415322\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/251597415322/locations/us-central1/trainingPipelines/3159232034308620288\n",
      "Model available at projects/251597415322/locations/us-central1/models/2045785419500486656\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"cymbal_custom_training_job\"\n",
    "MODEL_DISPLAY_NAME = \"cymbal_penguins_model\"\n",
    "\n",
    "# Use the `CustomTrainingJob` class to define the `TrainingPipeline`.\n",
    "container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"]\n",
    "model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "\n",
    "# [ TODO - Insert your code ]\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Define command-line arguments for the training script\n",
    "# Assumes LABEL_COLUMN is defined from the previous step\n",
    "CMDARGS = [\n",
    "    f\"--label_column={LABEL_COLUMN}\",\n",
    "]\n",
    "\n",
    "# Use the `CustomTrainingJob` class to define the `TrainingPipeline`.\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",  # <-- Assumes your training script is named task.py\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    model_serving_container_image_uri=model_serving_container_image_uri,\n",
    "    staging_bucket=\"gs://qwiklabs-gcp-02-4aecd326bf7c-cymbal\",\n",
    ")\n",
    "\n",
    "\n",
    "# Use the `run` function to start training\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    ")\n",
    "\n",
    "#[ TODO - Insert your code ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated"
   },
   "source": [
    "## Task 6. Deploy the model\n",
    "\n",
    "1. Create an `Endpoint` resource for deploying the `Model` resource to.\n",
    "2. Deploy the `Model` resource to the `Endpoint` resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WMH7GrYMlugy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/251597415322/locations/us-central1/endpoints/3297495844839751680/operations/678400787461701632\n",
      "Endpoint created. Resource name: projects/251597415322/locations/us-central1/endpoints/3297495844839751680\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/251597415322/locations/us-central1/endpoints/3297495844839751680')\n",
      "Endpoint created: projects/251597415322/locations/us-central1/endpoints/3297495844839751680\n",
      "Deploying model to Endpoint : projects/251597415322/locations/us-central1/endpoints/3297495844839751680\n",
      "Deploy Endpoint model backing LRO: projects/251597415322/locations/us-central1/endpoints/3297495844839751680/operations/1409954251932696576\n",
      "Endpoint model deployed. Resource name: projects/251597415322/locations/us-central1/endpoints/3297495844839751680\n",
      "Model deployed to endpoint: projects/251597415322/locations/us-central1/endpoints/3297495844839751680\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed\"\n",
    "ENDPOINT_DISPLAY_NAME = \"cymbal_penguins_model_endpoint\"\n",
    "\n",
    "# Deploy the model at model endpoint\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# 1. Create an Endpoint\n",
    "# This creates a resource that will host your deployed model.\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=ENDPOINT_DISPLAY_NAME\n",
    ")\n",
    "\n",
    "print(f\"Endpoint created: {endpoint.resource_name}\")\n",
    "\n",
    "# 2. Deploy the Model to the Endpoint\n",
    "# This provisions machine resources and deploys the model to the endpoint.\n",
    "# The 'model' variable is the one returned from the job.run() call in the previous step.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    machine_type=\"n1-standard-2\",  # A standard machine type for serving\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Task 7. Process the test data and make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "Prepare test data by convert it to a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "67aeea91384a"
   },
   "outputs": [],
   "source": [
    "df_holdout_y = df_holdout.pop(LABEL_COLUMN)\n",
    "df_holdout_x = df_holdout\n",
    "\n",
    "# Convert to list representation\n",
    "holdout_x = np.array(df_holdout_x).tolist()\n",
    "holdout_y = np.array(df_holdout_y).astype(\"float32\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of penguin measurement instances. According to your custom model, each instance should be an array of numbers. You prepared this list in the previous step.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the an instance in the request. In the output for each prediction, you see the following:\n",
    "\n",
    "- Confidence level for the prediction (`predictions`), between 0 and 1, for each of the ten classes.\n",
    "\n",
    "You can then run a quick evaluation on the prediction results:\n",
    "1. `np.argmax`: Convert each list of confidence levels to a label\n",
    "2. Print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6e20473b09f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = endpoint.predict(instances=holdout_x)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_output(bucket_name, blob_name, predicted_output):\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"w\") as f:\n",
    "        f.write(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_output(f\"{BUCKET_NAME}\", \"prediction.txt\", str(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "overview:custom",
    "objective:custom,training,online_prediction",
    "dataset:custom,cifar10,icn",
    "costs",
    "7c163842eabd",
    "accelerators:training,prediction",
    "container:training,prediction",
    "machine:training,prediction",
    "59f24e7d2269",
    "5c7732822757",
    "train_custom_model",
    "train_custom_job_args",
    "taskpy_contents",
    "train_custom_job",
    "deploy_model:dedicated",
    "make_prediction",
    "get_test_item:test",
    "send_prediction_request:image",
    "undeploy_model",
    "cleanup:custom"
   ],
   "name": "custom-tabular-bq-managed-dataset.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m137",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m137"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
